
# ğŸ”¥ **Mastering Gradient Descent in Python**  

ğŸ¯ A hands-on implementation of **Gradient Descent** for optimizing a **Linear Regression Model**!  

---

## ğŸš€ **Introduction**
Gradient Descent is the backbone of many machine learning algorithms. It helps optimize models by iteratively adjusting parameters (`m` and `b`) to minimize the **cost function**. This project provides a **simple yet effective** implementation to understand its working.  

---

## ğŸ§  **Concepts Covered**
âœ”ï¸ Understanding **Gradient Descent Algorithm**  
âœ”ï¸ Implementing **Linear Regression** from scratch  
âœ”ï¸ Optimizing the cost function using **partial derivatives**  
âœ”ï¸ Step-by-step updates of model parameters (`m` and `b`)  

---

## ğŸ“‚ **Project Structure**
```
ğŸ“¦ Gradient_Descent_Project
 â”£ ğŸ“œ Gradient_descent.py  # Python script implementing gradient descent
 â”£ ğŸ“œ README.md            # Project documentation
 â”£ ğŸ“œ requirements.txt      # Required dependencies
 â”— ğŸ“Š dataset.csv          # Sample dataset (if applicable)
```

---

## ğŸ”¢ **Understanding the Problem**
We aim to fit a straight line using **Gradient Descent** for a given dataset:  

| ğŸ  Area (sq ft) | ğŸ› Bedrooms | ğŸ— Age (years) | ğŸ’° Price ($) |
|------------|------------|------------|------------|
| 1000 | 2 | 10 | 150,000 |
| 1500 | 3 | 5  | 200,000 |
| 2000 | 3 | 15 | 250,000 |
| 2500 | 4 | 7  | 310,000 |

ğŸ“Œ Our goal is to **predict the price** of a new house based on its **features** using **Gradient Descent**.  

---

## ğŸ›  **Installation & Setup**
Make sure you have **Python 3.x** installed. Then, install the required libraries using:  

```bash
pip install numpy pandas matplotlib
```

---

## ğŸ— **How It Works**
1ï¸âƒ£ Initialize `m` and `b` to zero.  
2ï¸âƒ£ Compute `y_predicted = mx + b`.  
3ï¸âƒ£ Calculate the **cost function** using **Mean Squared Error (MSE)**:  
   \[
   J(m, b) = \frac{1}{n} \sum (y - \hat{y})^2
   \]  
4ï¸âƒ£ Compute **gradients** (partial derivatives) for `m` and `b`.  
5ï¸âƒ£ Update values iteratively using **learning rate (Î±)**:  
   \[
   m = m - \alpha \cdot \frac{\partial J}{\partial m}
   \]  
   \[
   b = b - \alpha \cdot \frac{\partial J}{\partial b}
   \]  
6ï¸âƒ£ Repeat until **convergence** (cost function stabilizes).  

---

## ğŸ“œ **Python Code**
Hereâ€™s the core implementation of **Gradient Descent**:

```python
import numpy as np
import matplotlib.pyplot as plt

def gradient_descent(x, y, learning_rate=0.008, iterations=10000):
    m_curr = b_curr = 0  # Initialize parameters
    n = len(x)  # Number of data points
    cost_history = []

    for i in range(iterations):
        y_predicted = m_curr * x + b_curr
        cost = (1/n) * sum((y - y_predicted) ** 2)  # Mean Squared Error
        md = -(2/n) * sum(x * (y - y_predicted))
        bd = -(2/n) * sum(y - y_predicted)
        m_curr -= learning_rate * md  # Update m
        b_curr -= learning_rate * bd  # Update b
        cost_history.append(cost)

        if i % 1000 == 0:
            print(f"Iteration {i}: m={m_curr:.4f}, b={b_curr:.4f}, Cost={cost:.4f}")

    return m_curr, b_curr, cost_history

# Sample Data
x = np.array([1, 2, 3, 4, 5])
y = np.array([5, 7, 9, 11, 13])

# Run Gradient Descent
m_final, b_final, cost_history = gradient_descent(x, y)

# Plot Cost Function
plt.plot(cost_history)
plt.xlabel("Iterations")
plt.ylabel("Cost")
plt.title("Cost Function Convergence")
plt.show()
```

---

## ğŸ“ˆ **Results & Observations**
âœ… The model optimizes `m` and `b` values to minimize the **cost function**.  
âœ… The cost function **decreases over time**, ensuring convergence.  
âœ… **Plotting cost vs. iterations** shows a smooth decline (good learning rate).  

---

## ğŸ”§ **Hyperparameters to Experiment With**
You can tweak the following parameters for **better model performance**:  

| Hyperparameter | Description | Suggested Values |
|---------------|-------------|------------------|
| Learning Rate (Î±) | Step size for parameter updates | `0.001 - 0.01` |
| Iterations | Number of times the algorithm updates `m` and `b` | `5000 - 50000` |
| Dataset Size | More data = better predictions | Varies |

---

## ğŸ¤ **Contributing**
ğŸ’¡ Found a bug? Have an idea for improvement? Feel free to **fork and contribute**! ğŸš€  

---
